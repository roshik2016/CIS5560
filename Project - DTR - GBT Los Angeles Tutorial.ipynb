{"cells":[{"cell_type":"markdown","source":["<a href=\"http://www.calstatela.edu/centers/hipic\"><img align=\"left\" src=\"https://avatars2.githubusercontent.com/u/4156894?v=3&s=100\"><image/>\n</a>\n<img align=\"right\" alt=\"California State University, Los Angeles\" src=\"http://www.calstatela.edu/sites/default/files/groups/California%20State%20University%2C%20Los%20Angeles/master_logo_full_color_horizontal_centered.svg\" style=\"width: 360px;\"/>\n\n# CIS5560 Term Project Tutorial\n# Predictive Analysis on Income"],"metadata":{}},{"cell_type":"markdown","source":["------\n#### Authors: [Roshik Ganesan](https://www.linkedin.com/in/roshik-ganesan-925143a1); [Kaustubh Padhya ](https://www.linkedin.com/in/kaustubhpadhya);[Mittal Vaghela](https://www.linkedin.com/in/mittal-vaghela-b2811177); [Manali Joshi](https://www.linkedin.com/in/manali-joshi-2a2b9a100)\n\n#### Instructor: [Jongwook Woo](https://www.linkedin.com/in/jongwook-woo-7081a85)\n\n#### Date: 05/18/2017"],"metadata":{}},{"cell_type":"markdown","source":["### Objectives\n\nThe aim of this Tutorial is to predict the Income of an employee based on the available features form the dataset by utilizing Machine Learning Algorithms and build accurate models using SparkML"],"metadata":{}},{"cell_type":"markdown","source":["### Prepare the Data\n\nFirst, import the dataset manually using the tables table in the left pane to upload the data, upon uploading the data give the table a name and select the apporpriate datatype for the data.\n\n- Numeric Values - Integer\n- Decimal Values - Float\n- Values Greater than 65000 - BigInt \n- Charater Values - String\n\nOn setting the appropriate datatype click on create table.\n\n First, Import the libraries you will need and prepare the training and test data."],"metadata":{}},{"cell_type":"code","source":["# Import Spark SQL and Spark ML libraries\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.regression import DecisionTreeRegressor"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Import the data from the table \n\nIn this step we are importing the data from the table using sql query and using the count function we find the total number of records in data."],"metadata":{}},{"cell_type":"code","source":["csv =sqlContext.sql(\"select * from salary2\")\nmaxval = sqlContext.sql(\"select count(PensionContributions) from salary2\")\nmaxval.show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Clipping Values from Data \n\nAs we do not have a clip model as seen in azure we need to find the row number for 90th percentile and 20th percentile which is to be clipped."],"metadata":{}},{"cell_type":"code","source":["csv =sqlContext.sql(\"select * from salary2\")\nmaxval_nine = sqlContext.sql(\"select count(PensionContributions)*.90 from salary2\")\nmaxval_twen = sqlContext.sql(\"select count(PensionContributions)*.20 from salary2\")\nmaxval_nine.show()\nmaxval_twen.show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["### Clipping Values from data\n\nAs we have found the row number for the clip we do the actual clipping of the data in this step. In this step we have ordered the data based on salary and we have clipped the values of the salary feature. In the following step we will be clipping the values of the Retirement, HealthDental and TotalCompensation and get the data ready for further processing."],"metadata":{}},{"cell_type":"code","source":["#Ordering the column PensionContribution and providing row numbers to data\nrownumber = sqlContext.sql(\"select ROW_NUMBER() over (ORDER BY PensionContributions) AS Row, PensionContributions from salary2\")\nrownumber.createOrReplaceTempView(\"res1\")\n#Finding the values corresponding to the row number which was found in the earlier step\nval_nineper = sqlContext.sql(\"select PensionContributions,Row from res1 where Row = '277110' \")\nval_twenper = sqlContext.sql(\"select PensionContributions,Row from res1 where Row = '61580' \")\nval_twenper.show()\nval_nineper.show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#Ordering the column MedicalDentalVision and providing row numbers to data\nrownumber = sqlContext.sql(\"select ROW_NUMBER() over (ORDER BY MedicalDentalVision) AS Row, MedicalDentalVision from salary2\")\nrownumber.createOrReplaceTempView(\"res1\")\n#Finding the values corresponding to the row number which was found in the earlier step\nval_nineper = sqlContext.sql(\"select MedicalDentalVision,Row from res1 where Row = '277110' \")\nval_twenper = sqlContext.sql(\"select MedicalDentalVision,Row from res1 where Row = '61580' \")\nval_twenper.show()\nval_nineper.show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["#Ordering the column TotalCompensation and providing row numbers to data\nrownumber = sqlContext.sql(\"select ROW_NUMBER() over (ORDER BY TotalCompensation) AS Row, TotalCompensation from salary2\")\nrownumber.createOrReplaceTempView(\"res1\")\n#Finding the values corresponding to the row number which was found in the earlier step\nval_nineper = sqlContext.sql(\"select TotalCompensation,Row from res1 where Row = '277110' \")\nval_twenper = sqlContext.sql(\"select TotalCompensation,Row from res1 where Row = '61580' \")\nval_twenper.show()\nval_nineper.show()"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["#Ordering the column LTDLifeMedicalTax and providing row numbers to data\nrownumber = sqlContext.sql(\"select ROW_NUMBER() over (ORDER BY LTDLifeMedicalTax) AS Row, LTDLifeMedicalTax from salary2\")\nrownumber.createOrReplaceTempView(\"res1\")\n#Finding the values corresponding to the row number which was found in the earlier step\nval_nineper = sqlContext.sql(\"select LTDLifeMedicalTax,Row from res1 where Row = '277110' \")\nval_twenper = sqlContext.sql(\"select LTDLifeMedicalTax,Row from res1 where Row = '61580' \")\nval_twenper.show()\nval_nineper.show()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Clipping Values from the data\n\nHere we pass the values of the feature that we have found for the corresponding row numbers."],"metadata":{}},{"cell_type":"code","source":["#Casting the data to a double datatype as there are a few inconsistant records in the data which would create an issue when the model is trained\ncsv1 = sqlContext.sql(\" select cast(PensionContributions as double),cast(MedicalDentalVision as double),cast(TotalCompensation as double),cast(LTDLifeMedicalTax as double) from salary2\")\n#Dropping null values\ncsv1= csv1.dropna()\n#Clipping the outliers from the data\ndata = csv1.select(\"PensionContributions\",\"MedicalDentalVision\",\"LTDLifeMedicalTax\", col(\"TotalCompensation\").alias(\"label\")).where(col(\"PensionContributions\") >= ((6321.92))).where (col(\"PensionContributions\") <= (24482.66)).where(col(\"MedicalDentalVision\") >= (6019.92)).where (col(\"MedicalDentalVision\") <= (18807.05)).where(col(\"TotalCompensation\") >= (57087.26)).where (col(\"TotalCompensation\") <= (177859.19)).where (col(\"LTDLifeMedicalTax\") >= (540.22)).where (col(\"LTDLifeMedicalTax\") <= (2303.66))\n\n# Split the data\nsplits = data.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1].withColumnRenamed(\"label\", \"trueLabel\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### Algorithm 1 - Decission Tree Algorithm\n\nA decission Tress algorithim is used for the first model to train the data for prediction\n\n### Create a Vector Assembler \n\nNow we create a vector assemble which would assemble all the selected feature and prepare it for pipeline"],"metadata":{}},{"cell_type":"code","source":["assembler = VectorAssembler(inputCols = [\"PensionContributions\",\"MedicalDentalVision\",\"LTDLifeMedicalTax\"], outputCol=\"features\")\ndt = DecisionTreeRegressor(labelCol =\"label\",featuresCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Tune Parameters\nYou can tune parameters to find the best model for your data. To do this you can use the  **CrossValidator** class to evaluate each combination of parameters defined in a **ParameterGrid** against multiple *folds* of the data split into training and validation datasets, in order to find the best performing parameters. Note that this can take a long time to run because every parameter combination is tried multiple times.\n\n### Why cross-validation: \nUsing one training set and one validation set could still end up over fitting your model that might not always produce the optimal model with the optimal parameters hence a cross validator is being used in this scenario"],"metadata":{}},{"cell_type":"code","source":["paramGrid = ParamGridBuilder()\\\n  .addGrid(dt.maxDepth, [5, 10])\\\n  .build()\n# We define an evaluation metric.  This tells CrossValidator how well we are doing by comparing the true labels with predictions.\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=dt.getLabelCol(), predictionCol=dt.getPredictionCol())\n# Declare the CrossValidator, which runs model tuning for us.\ncv = CrossValidator(estimator=dt, evaluator=evaluator, estimatorParamMaps=paramGrid)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Define the Pipeline\nNow define a pipeline that creates a feature vector and trains a regression model"],"metadata":{}},{"cell_type":"code","source":["# Define the pipeline\npipeline = Pipeline(stages=[assembler, cv])\npipelineModel = pipeline.fit(train)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["### Test the Model\nNow you're ready to apply the model to the test data."],"metadata":{}},{"cell_type":"code","source":["predictions = pipelineModel.transform(test)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["predicted = predictions.select(\"features\", \"prediction\", \"trueLabel\")\npredicted.show(100)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["### Plotting Predicted and Actual Values\n\nBased on the predection from the transform model the actual values and the predicted values are ploted in a scatter plot by creating a temporary view and we are close to acheving a diagonal line."],"metadata":{}},{"cell_type":"code","source":["predicted.createOrReplaceTempView(\"regressionPredictions\")"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":["# Reference: http://standarderror.github.io/notes/Plotting-with-PySpark/\ndataPred = spark.sql(\"SELECT trueLabel, prediction FROM regressionPredictions\")\n# convert to pandas and plot\ndisplay(dataPred)"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["### RMSE Analysis\n\nBeased on the RMSE (Root Mean Squared Error) this Model can be evaluated."],"metadata":{}},{"cell_type":"code","source":["evaluator = RegressionEvaluator(labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint \"Root Mean Square Error (RMSE) for Decession Tree Model:\", rmse"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["### Algorithm 2 - GBT Regressor (Gradient Booster Tree Regression)\n\nThe data is put to learn using a different machine learning algorithm (GBT) so that a comparison could be made and the best algorithm could be analyzed. The features are first assembled and then the model is trained and evaluated as done previously."],"metadata":{}},{"cell_type":"code","source":["assembler = VectorAssembler(inputCols = [\"PensionContributions\",\"MedicalDentalVision\",\"LTDLifeMedicalTax\"], outputCol=\"features\")\ngbt = GBTRegressor(labelCol=\"label\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["### Tune Parameters\nYou can tune parameters to find the best model for your data. To do this you can use the  **CrossValidator** class to evaluate each combination of parameters defined in a **ParameterGrid** against multiple *folds* of the data split into training and validation datasets, in order to find the best performing parameters. Note that this can take a long time to run because every parameter combination is tried multiple times."],"metadata":{}},{"cell_type":"code","source":["paramGrid = ParamGridBuilder()\\\n  .addGrid(gbt.maxDepth, [2, 5])\\\n  .addGrid(gbt.maxIter, [10, 100])\\\n  .build()\n# We define an evaluation metric.  This tells CrossValidator how well we are doing by comparing the true labels with predictions.\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol())\n# Declare the CrossValidator, which runs model tuning for us.\ncv = CrossValidator(estimator=gbt, evaluator=evaluator, estimatorParamMaps=paramGrid)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["### Define the Pipeline\nNow define a pipeline that creates a feature vector and trains a regression model"],"metadata":{}},{"cell_type":"code","source":["pipeline = Pipeline(stages=[assembler, cv])\npipelineModel = pipeline.fit(train)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":["predictions = pipelineModel.transform(test)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["predicted = predictions.select(\"features\", \"prediction\", \"trueLabel\")\ndisplay(predicted)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["### Plotting Predicted and Actual Values\n\nBased on the predection from the transform model the actual values and the predicted values are ploted in a scatter plot by creating a temporary view and we are close to acheving a diagonal line."],"metadata":{}},{"cell_type":"code","source":["predicted.createOrReplaceTempView(\"regressionPredictions\")"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# Reference: http://standarderror.github.io/notes/Plotting-with-PySpark/\ndataPred = spark.sql(\"SELECT trueLabel, prediction FROM regressionPredictions\")\n# convert to pandas and plot\ndisplay(dataPred)"],"metadata":{},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":["### RMSE Analysis\n\nBeased on the RMSE (Root Mean Squared Error) this algorithm can be evaluated."],"metadata":{}},{"cell_type":"code","source":["evaluator = RegressionEvaluator(labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint \"Root Mean Square Error (RMSE) for GBT Regression :\", rmse"],"metadata":{},"outputs":[],"execution_count":43}],"metadata":{"name":"Project - DTR - GBT Los Angeles Tutorial","notebookId":4188828901262290},"nbformat":4,"nbformat_minor":0}
