{"cells":[{"cell_type":"markdown","source":["<a href=\"http://www.calstatela.edu/centers/hipic\"><img align=\"left\" src=\"https://avatars2.githubusercontent.com/u/4156894?v=3&s=100\"><image/>\n</a>\n<img align=\"right\" alt=\"California State University, Los Angeles\" src=\"http://www.calstatela.edu/sites/default/files/groups/California%20State%20University%2C%20Los%20Angeles/master_logo_full_color_horizontal_centered.svg\" style=\"width: 360px;\"/>\n# CIS5560 Term Project Tutorial\n# Predictive Analysis on Income"],"metadata":{}},{"cell_type":"markdown","source":["------\n#### Authors: [Roshik Ganesan](https://www.linkedin.com/in/roshik-ganesan-925143a1); [Kaustubh Padhya](https://www.linkedin.com/in/kaustubhpadhya);[Mittal Vaghela](https://www.linkedin.com/in/mittal-vaghela-b2811177); [Manali Joshi](https://www.linkedin.com/in/manali-joshi-2a2b9a100)\n\n#### Instructor: [Jongwook Woo](https://www.linkedin.com/in/jongwook-woo-7081a85)\n\n#### Date: 05/18/2017"],"metadata":{}},{"cell_type":"markdown","source":["### Objectives\n\nThe aim of this Tutorial is to predict the Income of an employee based on the available features form the dataset by utilizing Machine Learning Algorithms and build accurate models using SparkML"],"metadata":{}},{"cell_type":"markdown","source":["### Prepare the Data\n\nFirst, import the dataset manually using the tables table in the left pane to upload the data, upon uploading the data give the table a name and select the apporpriate datatype for the data.\n\n- Numeric Values - Integer\n- Decimal Values - Float\n- Values Greater than 65000 - BigInt \n- Charater Values - String\n\nFirst, import the libraries you will need and prepare the training and test data:"],"metadata":{}},{"cell_type":"code","source":["# Import Spark SQL and Spark ML libraries\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.ml.regression import GBTRegressor\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator, RegressionEvaluator\nfrom pyspark.ml.tuning import CrossValidator, ParamGridBuilder\nfrom pyspark.ml.regression import DecisionTreeRegressor\nfrom pyspark.sql.functions import monotonically_increasing_id"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["### Import the data from the table \n\nIn this step we are importing the data from the table using sql query. As we do not have a clip model as seen in azure we need to find the row number 90th percentile and 20th percentile which is to be clipped."],"metadata":{}},{"cell_type":"code","source":["#Using Sql Context to import the data from the table.\ncsv =sqlContext.sql(\"select * from nysalary\")\ncsv.show(5)\n#Finding the row number for 90th and 20th percentile.\nmaxval_nine = sqlContext.sql(\"select count(Salaries)*.90 from nysalary\")\nmaxval_twen = sqlContext.sql(\"select count(Salaries)*.10 from nysalary\")\nmaxval_nine.show()\nmaxval_twen.show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["### Clipping Values from the data\n\nAs we have found the row number for the clip we do the actual clipping of the data in this step. In this step we have ordered the data based on salary and we have clipped the values of the salary feature. In the following step we will be clipping the values of the Retirement, HealthDental and TotalCompensation and get the data ready for further processing."],"metadata":{}},{"cell_type":"code","source":["#Ordering the column Salary and providing row numbers to data\nrownumber = sqlContext.sql(\"select ROW_NUMBER() over (ORDER BY Salaries) AS Row, Salaries from nysalary\")\nrownumber.createOrReplaceTempView(\"res1\")\n#finding the values corresponding to the row number which was found in the earlier step\nval_nineper = sqlContext.sql(\"select Salaries,Row from res1 where Row = '302751' \")\nval_twenper = sqlContext.sql(\"select Salaries,Row from res1 where Row = '33639' \")\nval_twenper.show()\nval_nineper.show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["#Ordering the column Retirement and providing row numbers to Column\nrownumber2 = sqlContext.sql(\"select ROW_NUMBER() over (ORDER BY Retirement) AS Row, Retirement from nysalary\")\nrownumber2.createOrReplaceTempView(\"res2\")\n#finding the values corresponding to the row number which was found in the earlier step\nret_nineper = sqlContext.sql(\"select Retirement,Row from res2 where Row = '302751' \")\nret_twenper = sqlContext.sql(\"select Retirement,Row from res2 where Row = '33639' \")\nret_twenper.show()\nret_nineper.show()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["#Ordering the column Retirement and providing row numbers to Column\nrownumber3 = sqlContext.sql(\"select ROW_NUMBER() over (ORDER BY HealthDental) AS Row, HealthDental from nysalary\")\nrownumber3.createOrReplaceTempView(\"res3\")\n#finding the values corresponding to the row number which was found in the earlier step\nhd_nineper = sqlContext.sql(\"select HealthDental,Row from res3 where Row = '302751' \")\nhd_twenper = sqlContext.sql(\"select HealthDental,Row from res3 where Row = '33639' \")\nhd_twenper.show()\nhd_nineper.show()"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["#Ordering the column TotalCompensation and providing row numbers to Column\nrownumber3 = sqlContext.sql(\"select ROW_NUMBER() over (ORDER BY TotalCompensation) AS Row, TotalCompensation from nysalary\")\nrownumber3.createOrReplaceTempView(\"res4\")\n#finding the values corresponding to the row number which was found in the earlier step\ntc_nineper = sqlContext.sql(\"select TotalCompensation,Row from res4 where Row = '302751' \")\ntc_twenper = sqlContext.sql(\"select TotalCompensation,Row from res4 where Row = '33639' \")\ntc_twenper.show()\ntc_nineper.show()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["### Clipping Values from the data\n\nHere we pass the values of the feature that we have found for the corresponding row numbers. \n\n### Split the data\n\nWe then split the data after clipping to perpare the test and the train data"],"metadata":{}},{"cell_type":"code","source":["#Casting the data to a double datatype as there are a few inconsistant records in the data which would create an issue when the model is trained\ncsv1 = sqlContext.sql(\" select cast(Salaries as double),cast(Retirement as double),cast(HealthDental as double),cast(TotalCompensation as double) from nysalary\")\n#Dropping null values\ncsv1 = csv1.dropna()\n#Clipping the outliers from the data\ndata = csv1.select(\"Salaries\",\"Retirement\",\"HealthDental\", col(\"TotalCompensation\").alias(\"label\")).where(col(\"Salaries\") >= ((4994.4))).where (col(\"Salaries\") <= (121743.54)).where(col(\"Retirement\") >= (0.0)).where (col(\"Retirement\") <= (25288.1)).where(col(\"HealthDental\") >= (764.58)).where (col(\"HealthDental\") <= (13489.25)).where (col(\"TotalCompensation\") >= (7934.39)).where (col(\"TotalCompensation\") <= (189487.16))\n\n# Split the data\nsplits = data.randomSplit([0.7, 0.3])\ntrain = splits[0]\ntest = splits[1].withColumnRenamed(\"label\", \"trueLabel\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Algorithm 1 - Decission Tree Algorithm\n\nA decission Tress algorithim is used for the first model to train the data for prediction\n### Create a Vector Assembler \n\nNow we create a vector assembler which would assemble all the selected feature and prepare it for pipeline"],"metadata":{}},{"cell_type":"code","source":["assembler = VectorAssembler(inputCols = [\"Salaries\",\"Retirement\",\"HealthDental\"], outputCol=\"features\")\ndt = DecisionTreeRegressor(featuresCol=\"features\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["### Tune Parameters\nYou can tune parameters to find the best model for your data. To do this you can use the  **CrossValidator** class to evaluate each combination of parameters defined in a **ParameterGrid** against multiple *folds* of the data split into training and validation datasets, in order to find the best performing parameters. Note that this can take a long time to run because every parameter combination is tried multiple times.\n\n### Why cross-validation: \nUsing one training set and one validation set could still end up over fitting your model that might not always produce the optimal model with the optimal parameters hence a cross validator is being used in this scenario"],"metadata":{}},{"cell_type":"code","source":["paramGrid = ParamGridBuilder()\\\n  .addGrid(dt.maxDepth, [5, 10])\\\n  .build()\n# We define an evaluation metric.  This tells CrossValidator how well we are doing by comparing the true labels with predictions.\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=dt.getLabelCol(), predictionCol=dt.getPredictionCol())\n# Declare the CrossValidator, which runs model tuning for us.\ncv = CrossValidator(estimator=dt, evaluator=evaluator, estimatorParamMaps=paramGrid)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["### Define the Pipeline\nNow define a pipeline that creates a feature vector and trains a regression model"],"metadata":{}},{"cell_type":"code","source":["# Define the pipeline\npipeline = Pipeline(stages=[assembler, cv])\npipelineModel = pipeline.fit(train)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["### Test the Model\nNow you're ready to apply the model to the test data."],"metadata":{}},{"cell_type":"code","source":["predictions = pipelineModel.transform(test)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["predicted = predictions.select(\"features\", \"prediction\", \"trueLabel\")\npredicted.show(100)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Examine the Predicted and Actual Values\nYou can plot the predicted values against the actual values to see how accurately the model has predicted. In a perfect model, the resulting scatter plot should form a perfect diagonal line with each predicted value being identical to the actual value - in practice, some variance is to be expected.\nRun the cells below to create a temporary table from the **predicted** DataFrame and then retrieve the predicted and actual label values using SQL. You can then display the results as a scatter plot, specifying **-** as the function to show the unaggregated values."],"metadata":{}},{"cell_type":"code","source":["predicted.createOrReplaceTempView(\"regressionPredictions\")"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# Reference: http://standarderror.github.io/notes/Plotting-with-PySpark/\ndataPred = spark.sql(\"SELECT trueLabel, prediction FROM regressionPredictions\")\n# convert to pandas and plot\n'''regressionPredictionsPanda = dataPred.toPandas()\nstuff = scatter_matrix(regressionPredictionsPanda, alpha=0.7, figsize=(6, 6), diagonal='kde')'''\ndisplay(dataPred)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["### RMSE Analysis\n\nBeased on the RMSE (Root Mean Squared Error) this Model can be evaluated."],"metadata":{}},{"cell_type":"code","source":["evaluator = RegressionEvaluator(labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint \"Root Mean Square Error (RMSE) for Decession Tree Model:\", rmse"],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":["### Algorithm 2 - GBT Regressor (Gradient Booster Tree Regression)\n\nThe data is put to learn using a different machine learning algorithm (GBT) so that a comparison could be made and the best algorithm could be analyzed. The features as first assembled and then the model is trained and evaluated as done previously."],"metadata":{}},{"cell_type":"code","source":["assembler = VectorAssembler(inputCols = [\"Salaries\",\"Retirement\",\"HealthDental\"], outputCol=\"features\")\ngbt = GBTRegressor(labelCol=\"label\")"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["### Tune Parameters\nYou can tune parameters to find the best model for your data. To do this you can use the  **CrossValidator** class to evaluate each combination of parameters defined in a **ParameterGrid** against multiple *folds* of the data split into training and validation datasets, in order to find the best performing parameters. Note that this can take a long time to run because every parameter combination is tried multiple times."],"metadata":{}},{"cell_type":"code","source":["paramGrid = ParamGridBuilder()\\\n  .addGrid(gbt.maxDepth, [2, 5])\\\n  .addGrid(gbt.maxIter, [10, 100])\\\n  .build()\n# We define an evaluation metric.  This tells CrossValidator how well we are doing by comparing the true labels with predictions.\nevaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=gbt.getLabelCol(), predictionCol=gbt.getPredictionCol())\n# Declare the CrossValidator, which runs model tuning for us.\ncv = CrossValidator(estimator=gbt, evaluator=evaluator, estimatorParamMaps=paramGrid)"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["### Define the Pipeline\nNow define a pipeline that creates a feature vector and trains a regression model"],"metadata":{}},{"cell_type":"code","source":["pipeline = Pipeline(stages=[assembler, cv])\npipelineModel = pipeline.fit(train)"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":["predictions = pipelineModel.transform(test)"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":["predicted = predictions.select(\"features\", \"prediction\", \"trueLabel\")\npredicted.show(100)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["### Examine the Predicted and Actual Values\nYou can plot the predicted values against the actual values to see how accurately the model has predicted. In a perfect model, the resulting scatter plot should form a perfect diagonal line with each predicted value being identical to the actual value - in practice, some variance is to be expected.\nRun the cells below to create a temporary table from the **predicted** DataFrame and then retrieve the predicted and actual label values using SQL. You can then display the results as a scatter plot, specifying **-** as the function to show the unaggregated values."],"metadata":{}},{"cell_type":"code","source":["predicted.createOrReplaceTempView(\"regressionPredictions\")"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"code","source":["# Reference: http://standarderror.github.io/notes/Plotting-with-PySpark/\ndataPred = spark.sql(\"SELECT trueLabel, prediction FROM regressionPredictions\")\n# convert to pandas and plot\ndisplay(dataPred)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["### RMSE Analysis\n\nBeased on the RMSE (Root Mean Squared Error) this algorithm can be evaluated."],"metadata":{}},{"cell_type":"code","source":["evaluator = RegressionEvaluator(labelCol=\"trueLabel\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = evaluator.evaluate(predictions)\nprint \"Root Mean Square Error (RMSE) for GBT Regression :\", rmse"],"metadata":{},"outputs":[],"execution_count":41}],"metadata":{"name":"Project - DTR - GBT San Francisco Tutiorial","notebookId":4188828901262247},"nbformat":4,"nbformat_minor":0}
